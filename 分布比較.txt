
離散分布における予測確率分布と真の確率分布を比較するための指標

1. クロスエントロピー損失（Cross-Entropy Loss）
   概要: 主に分類問題で使用される指標で、予測分布と真の分布の間の情報量の差を測定します。
   式: H(p, q) = -∑ p(i) log q(i)
   特徴: 真の分布が1つのクラスに集中している場合、予測分布が正しいクラスにどれだけ確信を持っているかを評価します。

2. Kullback-Leibler Divergence (KLダイバージェンス)
   概要: 予測分布 q が真の分布 p にどれだけ近いかを測定します。
   式: D_KL(p || q) = ∑ p(i) log (p(i) / q(i))
   特徴: 非対称であり、真の分布 p が与えられたときに予測分布 q がどれだけ情報を失っているかを測定します。

3. Jensen-Shannon Divergence (JSダイバージェンス)
   概要: KLダイバージェンスを対称化したもの。
   式: D_JS(p || q) = 1/2 * D_KL(p || m) + 1/2 * D_KL(q || m), ここで m = (p + q) / 2
   特徴: KLダイバージェンスとは異なり、対称であるため解釈が容易です。

   mが0になることはないので、KLダイバージェンスが発散する事態を避けることができる。

4. Hellinger Distance
   概要: 確率分布間の距離を測定する指標。
   式: H(p, q) = (1/√2) * √(∑ (√p(i) - √q(i))^2)
   特徴: 値は 0 から 1 の間に収まり、分布が近いほど小さくなります。

5. Total Variation Distance（全変動距離）
   概要: 2つの確率分布間の最大差異を測定。
   式: TV(p, q) = (1/2) * ∑ |p(i) - q(i)|
   特徴: 分布の距離を明確に数値化し、直感的に理解しやすい。
